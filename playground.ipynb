{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444daf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0443e6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   418.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 24 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>3.40e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:31:16</td>     <th>  Log-Likelihood:    </th> <td>  3.5098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    10</td>      <th>  AIC:               </th> <td>  -3.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     8</td>      <th>  BIC:               </th> <td>  -2.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.5400</td> <td>    0.130</td> <td>   11.837</td> <td> 0.000</td> <td>    1.240</td> <td>    1.840</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.4291</td> <td>    0.021</td> <td>   20.464</td> <td> 0.000</td> <td>    0.381</td> <td>    0.477</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.353</td> <th>  Durbin-Watson:     </th> <td>   0.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.838</td> <th>  Jarque-Bera (JB):  </th> <td>   0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.328</td> <th>  Prob(JB):          </th> <td>   0.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.250</td> <th>  Cond. No.          </th> <td>    13.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.981   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.979   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     418.8   \\\\\n",
       "\\textbf{Date:}             & Mon, 24 Nov 2025 & \\textbf{  Prob (F-statistic):} &  3.40e-08   \\\\\n",
       "\\textbf{Time:}             &     20:31:16     & \\textbf{  Log-Likelihood:    } &    3.5098   \\\\\n",
       "\\textbf{No. Observations:} &          10      & \\textbf{  AIC:               } &    -3.020   \\\\\n",
       "\\textbf{Df Residuals:}     &           8      & \\textbf{  BIC:               } &    -2.414   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       1.5400  &        0.130     &    11.837  &         0.000        &        1.240    &        1.840     \\\\\n",
       "\\textbf{x1}    &       0.4291  &        0.021     &    20.464  &         0.000        &        0.381    &        0.477     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.353 & \\textbf{  Durbin-Watson:     } &    0.523  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.838 & \\textbf{  Jarque-Bera (JB):  } &    0.414  \\\\\n",
       "\\textbf{Skew:}          &  0.328 & \\textbf{  Prob(JB):          } &    0.813  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.250 & \\textbf{  Cond. No.          } &     13.7  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.981\n",
       "Model:                            OLS   Adj. R-squared:                  0.979\n",
       "Method:                 Least Squares   F-statistic:                     418.8\n",
       "Date:                Mon, 24 Nov 2025   Prob (F-statistic):           3.40e-08\n",
       "Time:                        20:31:16   Log-Likelihood:                 3.5098\n",
       "No. Observations:                  10   AIC:                            -3.020\n",
       "Df Residuals:                       8   BIC:                            -2.414\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.5400      0.130     11.837      0.000       1.240       1.840\n",
       "x1             0.4291      0.021     20.464      0.000       0.381       0.477\n",
       "==============================================================================\n",
       "Omnibus:                        0.353   Durbin-Watson:                   0.523\n",
       "Prob(Omnibus):                  0.838   Jarque-Bera (JB):                0.414\n",
       "Skew:                           0.328   Prob(JB):                        0.813\n",
       "Kurtosis:                       2.250   Cond. No.                         13.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = {\n",
    "    \"i\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"y\": [2.3, 2.5, 2.7, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0],\n",
    "}  # --- IGNORE\n",
    "X = sm.add_constant(df[\"i\"])  # adding a constant\n",
    "model = sm.OLS(df[\"y\"], X).fit()\n",
    "predictions = model.predict(X)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc1debbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.426e+30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 24 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>7.97e-76</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:31:16</td>     <th>  Log-Likelihood:    </th> <td>  291.23</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     9</td>      <th>  AIC:               </th> <td>  -574.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     5</td>      <th>  BIC:               </th> <td>  -573.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.0000</td> <td> 1.06e-14</td> <td> 9.39e+13</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 4.025e-16</td> <td> 1.68e-15</td> <td>    0.240</td> <td> 0.820</td> <td>-3.91e-15</td> <td> 4.71e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>-4.996e-16</td> <td> 1.86e-15</td> <td>   -0.269</td> <td> 0.799</td> <td>-5.27e-15</td> <td> 4.28e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    1.0000</td> <td> 2.62e-15</td> <td> 3.81e+14</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.906</td> <th>  Durbin-Watson:     </th> <td>   0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.636</td> <th>  Jarque-Bera (JB):  </th> <td>   0.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.138</td> <th>  Prob(JB):          </th> <td>   0.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.764</td> <th>  Cond. No.          </th> <td>    99.0</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     1.000   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     1.000   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 2.426e+30   \\\\\n",
       "\\textbf{Date:}             & Mon, 24 Nov 2025 & \\textbf{  Prob (F-statistic):} &  7.97e-76   \\\\\n",
       "\\textbf{Time:}             &     20:31:16     & \\textbf{  Log-Likelihood:    } &    291.23   \\\\\n",
       "\\textbf{No. Observations:} &           9      & \\textbf{  AIC:               } &    -574.5   \\\\\n",
       "\\textbf{Df Residuals:}     &           5      & \\textbf{  BIC:               } &    -573.7   \\\\\n",
       "\\textbf{Df Model:}         &           3      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       1.0000  &     1.06e-14     &  9.39e+13  &         0.000        &        1.000    &        1.000     \\\\\n",
       "\\textbf{x1}    &    4.025e-16  &     1.68e-15     &     0.240  &         0.820        &    -3.91e-15    &     4.71e-15     \\\\\n",
       "\\textbf{x2}    &   -4.996e-16  &     1.86e-15     &    -0.269  &         0.799        &    -5.27e-15    &     4.28e-15     \\\\\n",
       "\\textbf{x3}    &       1.0000  &     2.62e-15     &  3.81e+14  &         0.000        &        1.000    &        1.000     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.906 & \\textbf{  Durbin-Watson:     } &    0.105  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.636 & \\textbf{  Jarque-Bera (JB):  } &    0.602  \\\\\n",
       "\\textbf{Skew:}          & -0.138 & \\textbf{  Prob(JB):          } &    0.740  \\\\\n",
       "\\textbf{Kurtosis:}      &  1.764 & \\textbf{  Cond. No.          } &     99.0  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       1.000\n",
       "Model:                            OLS   Adj. R-squared:                  1.000\n",
       "Method:                 Least Squares   F-statistic:                 2.426e+30\n",
       "Date:                Mon, 24 Nov 2025   Prob (F-statistic):           7.97e-76\n",
       "Time:                        20:31:16   Log-Likelihood:                 291.23\n",
       "No. Observations:                   9   AIC:                            -574.5\n",
       "Df Residuals:                       5   BIC:                            -573.7\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.0000   1.06e-14   9.39e+13      0.000       1.000       1.000\n",
       "x1          4.025e-16   1.68e-15      0.240      0.820   -3.91e-15    4.71e-15\n",
       "x2         -4.996e-16   1.86e-15     -0.269      0.799   -5.27e-15    4.28e-15\n",
       "x3             1.0000   2.62e-15   3.81e+14      0.000       1.000       1.000\n",
       "==============================================================================\n",
       "Omnibus:                        0.906   Durbin-Watson:                   0.105\n",
       "Prob(Omnibus):                  0.636   Jarque-Bera (JB):                0.602\n",
       "Skew:                          -0.138   Prob(JB):                        0.740\n",
       "Kurtosis:                       1.764   Cond. No.                         99.0\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = {\n",
    "    \"x1\": [0, 3, 3, 3, 5, 7, 8, 9, 10],\n",
    "    \"x2\": [5, 4, 4, 2, 3, 3, 2, 1, 0],\n",
    "    \"x3\": [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    \"y\": [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "}  # --- IGNORE\n",
    "X = sm.add_constant(pd.DataFrame(df, columns=[\"x1\", \"x2\", \"x3\"]))\n",
    "model = sm.OLS(df[\"y\"], X).fit()\n",
    "predictions = model.predict(X)\n",
    "model.summary()\n",
    "\n",
    "# design_matrix = model.model.exog\n",
    "# design_matrix\n",
    "\n",
    "# Xcopy = design_matrix.copy()\n",
    "\n",
    "# Xt = design_matrix.T\n",
    "# Xtx = Xt @ design_matrix\n",
    "# detXtx = np.linalg.det(Xtx)\n",
    "# assert detXtx != 0, \"Matrix is singular and cannot be inverted\"\n",
    "# Xtx_inv = np.linalg.inv(Xtx)\n",
    "# Xtx_inv_Xt = Xtx_inv @ Xt\n",
    "# coefficients = Xtx_inv_Xt @ df[\"y\"]\n",
    "# coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673925c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant: -3.996802888650571e-13\n"
     ]
    }
   ],
   "source": [
    "matrix = [[5, 15, 15], [15, 55, 35], [15, 35, 55]]\n",
    "det = np.linalg.det(matrix)\n",
    "print(\"Determinant:\", det)\n",
    "assert det != 0, \"Matrix is singular and cannot be inverted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "948a27f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient:\n",
      " [[ -5.5       ]\n",
      " [-17.16666667]\n",
      " [-22.66666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array(\n",
    "    [[1, 2, 3], [1, 3, 4], [1, 4, 5]]\n",
    ")  # includes bias column if intercept is used\n",
    "y = np.array([[10], [13], [16]])\n",
    "\n",
    "# Parameters (random initialization)\n",
    "beta = np.array([[0.5], [1.0], [1.0]])\n",
    "\n",
    "# Predictions\n",
    "y_hat = X @ beta\n",
    "\n",
    "# Gradient of MSE\n",
    "n = len(y)\n",
    "gradient = -(1 / n) * X.T @ (y - y_hat)\n",
    "\n",
    "print(\"Gradient:\\n\", gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b6804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso intercept: 2.5\n",
      "Lasso coefficients: [1.11803399]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml/lib/python3.13/site-packages/sklearn/base.py:1365: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/anaconda3/envs/ml/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# sample data\n",
    "x = [1, 2, 3, 4]\n",
    "y = [1, 2, 3, 4]\n",
    "lmb = 0\n",
    "\n",
    "# fit a lasso regression model\n",
    "\n",
    "model = make_pipeline(StandardScaler(), Lasso(alpha=lmb))\n",
    "# we use reshape to convert x to a 2D array with one column\n",
    "xmatrix = np.array(x).reshape(-1, 1)\n",
    "model.fit(xmatrix, y)\n",
    "# intercept\n",
    "print(\"Lasso intercept:\", model.named_steps[\"lasso\"].intercept_)\n",
    "print(\"Lasso coefficients:\", model.named_steps[\"lasso\"].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc68225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso intercept: 3.9\n",
      "Lasso coefficients: [0.2324698]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# sample data\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "y = [2.3, 2.5, 2.7, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0]\n",
    "lmb = 1\n",
    "\n",
    "# fit a lasso regression model\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(), Lasso(alpha=lmb, fit_intercept=True, max_iter=10000000)\n",
    ")\n",
    "# we use reshape to convert x to a 2D array with one column\n",
    "xmatrix = np.array(x).reshape(-1, 1)\n",
    "model.fit(xmatrix, y)\n",
    "# intercept\n",
    "print(\"Lasso intercept:\", model.named_steps[\"lasso\"].intercept_)\n",
    "print(\"Lasso coefficients:\", model.named_steps[\"lasso\"].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67aacd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X mean: [5.5]\n",
      "X std: [2.87228132]\n",
      "First 3 normalized X values: [-1.5666989  -1.21854359 -0.87038828]\n",
      "\n",
      "Lasso intercept: 3.9\n",
      "Lasso coefficient: 0.23246980416634067\n"
     ]
    }
   ],
   "source": [
    "# PROPOSED\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([2.3, 2.5, 2.7, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0])\n",
    "\n",
    "# Manually check StandardScaler normalization\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "print(\"X mean:\", scaler.mean_)\n",
    "print(\"X std:\", scaler.scale_)\n",
    "print(\"First 3 normalized X values:\", x_scaled[:3].flatten())\n",
    "\n",
    "# Fit Lasso\n",
    "model = Lasso(alpha=1.0, fit_intercept=True, max_iter=10000000)\n",
    "model.fit(x_scaled, y)\n",
    "\n",
    "print(\"\\nLasso intercept:\", model.intercept_)\n",
    "print(\"Lasso coefficient:\", model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ab039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy std (ddof=0, population): 2.8722813232690143\n",
      "NumPy std (ddof=1, sample): 3.0276503540974917\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "obs = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "print(\"NumPy std (ddof=0, population):\", np.std(obs))\n",
    "print(\"NumPy std (ddof=1, sample):\", np.std(obs, ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5de44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso intercept: 3.9\n",
      "Lasso coefficients:\n",
      "coef: 0.0295\n",
      "coef: 1.2239\n",
      "coef: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml/lib/python3.13/site-packages/sklearn/base.py:1365: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/anaconda3/envs/ml/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/ml/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.411e-01, tolerance: 1.548e-03\n",
      "Linear regression models with a zero l1 penalization strength are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# sample data, x 2 dimensional\n",
    "x = [\n",
    "    [3, 1, 2],\n",
    "    [3, 2, 3],\n",
    "    [4, 3, 4],\n",
    "    [3, 4, 5],\n",
    "    [2, 5, 6],\n",
    "    [3, 6, 7],\n",
    "    [4, 7, 8],\n",
    "    [3, 8, 9],\n",
    "    [5, 9, 10],\n",
    "    [3, 10, 11],\n",
    "]\n",
    "y = [2.3, 2.5, 2.7, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0]\n",
    "lmb = 0\n",
    "\n",
    "# fit a lasso regression model\n",
    "\n",
    "model = make_pipeline(StandardScaler(), Lasso(alpha=lmb))\n",
    "# we use reshape to convert x to a 2D array with one column\n",
    "X = np.array(x)\n",
    "\n",
    "model.fit(X, y)\n",
    "# intercept\n",
    "print(\"Lasso intercept:\", model.named_steps[\"lasso\"].intercept_)\n",
    "print(\n",
    "    \"Lasso coefficients:\",\n",
    ")\n",
    "for coef in model.named_steps[\"lasso\"].coef_:\n",
    "    print(f\"coef: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d56b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso intercept: 3.9\n",
      "Lasso coefficients:\n",
      "coef: 0.2325\n",
      "coef: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# sample data, x 2 dimensional\n",
    "x = [\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [4, 5],\n",
    "    [5, 6],\n",
    "    [6, 7],\n",
    "    [7, 8],\n",
    "    [8, 9],\n",
    "    [9, 10],\n",
    "    [10, 11],\n",
    "]\n",
    "y = [2.3, 2.5, 2.7, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0]\n",
    "lmb = 1\n",
    "\n",
    "# fit a lasso regression model\n",
    "\n",
    "model = make_pipeline(StandardScaler(), Lasso(alpha=lmb))\n",
    "# we use reshape to convert x to a 2D array with one column\n",
    "X = np.array(x)\n",
    "\n",
    "model.fit(X, y)\n",
    "# intercept\n",
    "print(\"Lasso intercept:\", model.named_steps[\"lasso\"].intercept_)\n",
    "print(\n",
    "    \"Lasso coefficients:\",\n",
    ")\n",
    "for coef in model.named_steps[\"lasso\"].coef_:\n",
    "    print(f\"coef: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bea178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized X:\n",
      "[[-1.5666989  -1.5666989 ]\n",
      " [-1.21854359 -1.21854359]\n",
      " [-0.87038828 -0.87038828]\n",
      " [-0.52223297 -0.52223297]\n",
      " [-0.17407766 -0.17407766]\n",
      " [ 0.17407766  0.17407766]\n",
      " [ 0.52223297  0.52223297]\n",
      " [ 0.87038828  0.87038828]\n",
      " [ 1.21854359  1.21854359]\n",
      " [ 1.5666989   1.5666989 ]]\n",
      "\n",
      "Correlation between features:\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "Intercept: 3.9\n",
      "Coefficients: [0.2324698 0.       ]\n",
      "Sum of coefficients: 0.23246980416634067\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your test data\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 4],\n",
    "        [4, 5],\n",
    "        [5, 6],\n",
    "        [6, 7],\n",
    "        [7, 8],\n",
    "        [8, 9],\n",
    "        [9, 10],\n",
    "        [10, 11],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array([2.3, 2.5, 2.7, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0])\n",
    "\n",
    "# Normalize features (like your NormalizeObservations does)\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Normalized X:\")\n",
    "print(X_normalized)\n",
    "print(\"\\nCorrelation between features:\")\n",
    "print(np.corrcoef(X_normalized.T))\n",
    "\n",
    "# Fit Lasso\n",
    "lasso = Lasso(alpha=1.0, max_iter=1000000, tol=1e-6, fit_intercept=True)\n",
    "lasso.fit(X_normalized, y)\n",
    "\n",
    "print(f\"\\nIntercept: {lasso.intercept_}\")\n",
    "print(f\"Coefficients: {lasso.coef_}\")\n",
    "print(f\"Sum of coefficients: {lasso.coef_.sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
